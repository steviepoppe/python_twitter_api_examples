import json
import tweepy
import sys
import csv
import os
import pandas as pd
import ijson

from datetime import date, datetime
from pytz import timezone
from pathlib import Path

# This python script takes the JSON output generated by either of the Twitter API datamining scripts and exports a number of relevant fields of each tweet to a CSV file.
# Saved data includes: tweet ID, text, hashtags, user mentions, time of creation, information on the poster (user name, description, followers and followings, total tweets), 
# whether the tweet is a retweet (if so, user name of the original poster)

# The script used the third-party python libraries ijson and pandas.
# ijson is used to parse particularly large JSON files in chunks
# pandas is used to parse particularly large CSV files in chunks

# The script contains an optional method to find and remove duplicate rows, to localize the UTC format to a timezone of choice, and
# a method to calculate the total amount each tweet has been retweeted in our dataset (important for datasets collected over longer times)


def parse_tweets(sys_args):
	file_name = sys_args[1]
	time_zone = sys_args[2] if len(sys_args) > 2 else "utc"
	keep_rt = sys_args[3] if len(sys_args) > 3 else "True"
	save_file_name = sys_args[4] if len(sys_args) > 4 else ('%s_parsed' % file_name)
	add_totals = sys_args[5] if len(sys_args) > 5 else "False"
	check_duplicates = sys_args[6] if len(sys_args) > 6 else "False"
	count_tweets = 0
	exists = os.path.isfile('./results/%s.csv' % save_file_name)

	tweet_list = list()

	with open("./results/%s.json" % file_name, mode='r', encoding="utf-8") as tweet_data:	
		tweets = ijson.items(tweet_data, 'objects.item')
		for tweet in tweets:
			entities = tweet["entities"]
			user = tweet["user"]
			hashtags = ()
			user_mentions = ()

			if 'user_mentions' in entities:
				user_mentions = (user_mention["screen_name"] for user_mention in entities["user_mentions"])
	
			tweet_row = {}
			tweet_row["tweet_id"] = tweet["id"]
			tweet_row["text"] = ""
			tweet_row["hashtags"] = ""			
			tweet_row["user_mentions"] = ','.join(user_mentions)
			tweet_row["created_at"] = localize_utc_object(tweet["created_at"],time_zone)			

			is_retweet = ("retweeted_status" in tweet)
			if keep_rt == "True":
				tweet_row["is_retweet"] = is_retweet
				tweet_row["retweet_id"] = 0
				tweet_row["retweet_created_at"] = None	

			if keep_rt == "True" and is_retweet == True:
				retweet = tweet["retweeted_status"]
				re_entities = retweet["entities"]
				tweet_row["retweet_id"] = retweet["id"]
				tweet_row["retweet_created_at"] = localize_utc_object(retweet["created_at"],time_zone)
					
				tweet_row["text"] = "RT @" + retweet["user"]["screen_name"] + ": " + (retweet["extended_tweet"]["full_text"] if "extended_tweet" in retweet 
					else retweet["full_text"] if "full_text" in retweet else retweet["text"])
						
				if 'hashtags' in re_entities:
					hashtags = (hashtag["text"] for hashtag in re_entities["hashtags"])
				if len(re_entities['user_mentions']) > 0:
					re_user_mentions = (re_user_mention["screen_name"] for re_user_mention in re_entities["user_mentions"] if re_user_mention["screen_name"] not in user_mentions)
					re_user_mentions = ','.join(re_user_mentions)
					tweet_row["user_mentions"] += ("," + re_user_mentions)			
			else:					
				tweet_row["text"] = (tweet["extended_tweet"]["full_text"] if "extended_tweet" in tweet else tweet["full_text"] if "full_text" in tweet else tweet["text"])		

				if 'hashtags' in entities:
					hashtags = (hashtag["text"] for hashtag in entities["hashtags"])

			tweet_row["hashtags"] = ','.join(hashtags)			
			tweet_row["text"] = tweet_row["text"].strip()

			tweet_row["user_screen_name"] = user["screen_name"]
			tweet_row["user_description"] = user["description"]
			tweet_row["user_following_count"] = user["friends_count"]
			tweet_row["user_followers_count"] = user["followers_count"]
			tweet_row["user_total_tweets"] = user["statuses_count"]
			tweet_row["user_created_at"] = localize_utc_object(user["created_at"],time_zone)
			tweet_row["retweet_count_listed"] = tweet["retweet_count"]
			tweet_row["retweet_count_dataset"] = 0

			count_tweets += 1

			tweet_list.append(tweet_row)

	#these are memory intensive, do some garbage collecting
	del tweets
	del tweet_data

	tweet_json = pd.DataFrame(tweet_list)	
	#tweet_json = pd.DataFrame(tweet_list).sort_values(by=["created_at"], ascending=True)

	with open('./results/%s.csv' % save_file_name, mode='a', encoding="utf-8",newline='') as file:
		tweet_json.to_csv(file, header=(not exists), index = False)

	del tweet_json
	del file

	if check_duplicates == "True":
	 	remove_duplicate_rows(save_file_name)

	print("Finished processing %s tweets. Saved to ./results/%s.csv" % (count_tweets, save_file_name))

	if add_totals == "True":
	 	get_retweet_count(save_file_name)

#note, changing the timestring to match the location based on an estimation (by language filters for example) is a bit sloppy:
#-> what about Japanese tweets posted abroad? Difficult to detect since utc_offset is no longer supported by Twitter
#https://en.wikipedia.org/wiki/List_of_tz_database_time_zones
def localize_utc_object(time_string, time_zone):
	date_object =  datetime.strptime(time_string, '%a %b %d %H:%M:%S %z %Y')
	if time_zone != "utc":	
		date_object = date_object.astimezone(tz=timezone(time_zone)) #for example Asia/Tokyo
	return date_object.isoformat()


# Only way to find duplicates in CSV files that are too large too load in one time is to iterate by
#  chunks, collecting doubles in the first iteration and removing them in the second one
def remove_duplicate_rows(save_fn):
	double_ids = []
	chunksize = 100000
	count = 0
	header = True

	for chunk in pd.read_csv('./results/%s.csv' % save_fn, chunksize=chunksize, iterator=True):
		double_ids.extend(chunk['tweet_id'].tolist())

	#Get list of double tweet_id
	seen = set()
	seen2 = []
	seen_add = seen.add
	seen2_add = seen2.append
	for tweet in double_ids:
		if tweet in seen:
			seen2_add(tweet)
		else:
			seen_add(tweet)
	del seen

	os.rename("./results/%s.csv" % save_fn,"./results/%s_temp.csv" % save_fn)
	for chunk in pd.read_csv('./results/%s_temp.csv' % save_fn, chunksize=chunksize, iterator=True):
		for index, tweet in chunk.iterrows():
			if tweet["tweet_id"] in seen2:
				chunk.drop(index, inplace=True)
				count += 1
				seen2.remove(tweet["tweet_id"])
				print('Tweet ID: %s dropped from CSV.' % tweet["tweet_id"])
		chunk.to_csv('./results/%s.csv' % save_fn, encoding="utf-8",header=header, index = False, mode='a')
		header = False

	print('%d duplicate row(s) removed.' % count)
	os.remove("./results/%s_temp.csv" % save_fn)

def get_retweet_count(file_name):

	chunksize = 100000
	header = True
	line_count = 0
	Path("./results/metrics_%s/" % file_name).mkdir(parents=True, exist_ok=True)

	os.rename("./results/%s.csv" % file_name,"./results/%s_temp.csv" % file_name)

	df = pd.read_csv('./results/%s_temp.csv' % file_name, usecols = ["retweet_id"])
	retweets = df['retweet_id'].value_counts().to_dict()
	if 0 in retweets:
		del retweets[0]

	print("%s unique retweets found. Processing..." % len(retweets))
	for chunk in pd.read_csv('./results/%s_temp.csv' % file_name, chunksize=chunksize, iterator=True):
		for index, tweet in chunk.iterrows():
			if tweet["tweet_id"] in retweets:
				chunk.at[index,'retweet_count_dataset'] = retweets[tweet["tweet_id"]]
				del retweets[tweet["tweet_id"]]
		chunk.to_csv('./results/%s.csv' % file_name, encoding="utf-8",header=header, index = False, mode='a')
		header = False

	os.remove("./results/%s_temp.csv" % file_name)		

	print("%s retweets not in data set. Processing..." % len(retweets))

	with open('./results/metrics_%s/%s_old_retweets.csv' % (file_name, file_name), mode='w', encoding="utf-8",newline='') as file:
		writer = csv.writer(file)				
		writer.writerow(["tweet_id","text", "hashtags", "user_screen_name", 
			"user_mentions", "created_at", "retweet_count", "url"])

		for chunk in pd.read_csv('./results/%s.csv' % file_name, usecols = ["tweet_id","text", "hashtags","user_mentions", "retweet_created_at", "retweet_id"], chunksize=chunksize, iterator=True):
			for index, tweet in chunk.iterrows():
				if tweet["retweet_id"] in retweets:
					if pd.isna(tweet["user_mentions"]):
						user_mentions = re.findall(r'((?<=^|(?<=[^a-zA-Z0-9-\.]))(?<=@)[a-zA-Z0-9\/_]+(?=[^a-zA-Z0-9-_\.]))',tweet["text"])
					else:
						user_mentions = tweet["user_mentions"].split(",")
					screenname = user_mentions[0]
					f_text = "RT @%s: " % screenname
					## There's a rare flaw in the Twitter API that does not return the user mention of the original tweeter
					## In such cases, use a regular expression
					f_mentions = ",".join(user_mentions[1:]) if len(user_mentions[1:]) > 0 else None
					hashtags = None if pd.isna(tweet["hashtags"]) else tweet["hashtags"]
					url = "https://twitter.com/%s/status/%s" % (screenname, str(tweet["retweet_id"]))
					writer.writerow([tweet["retweet_id"],tweet["text"].replace(f_text,''), hashtags, 
					screenname, f_mentions, tweet["retweet_created_at"], retweets[tweet["retweet_id"]], url])		
					del retweets[tweet["retweet_id"]]
					line_count += 1

	print("Finished calculating total tweets. Processed %s old retweets." % line_count)

if __name__ == '__main__':
	parse_tweets(sys.argv)
